# ===========================================
# CodeRAG Configuration for Server
# ===========================================
# Используем уменьшенную выборку для тестирования
sample_n = 100 # max 6461, 2665 or None for all
log_level = "INFO"

[benchmark]
name = "cceval"
repos_path = "cache/benchmark/cceval/raw_data"
meta_data_path = "cache/benchmark/cceval/python/line_completion.jsonl"
# data_indices_path = "benchmark/cceval/test_indices.json" #use full benchmark or just these indices

# Альтернатива: использовать ReccEval
# name = "recceval"
# repos_path = "cache/benchmark/ReccEval/Source_Code"
# meta_data_path = "cache/benchmark/ReccEval/metadata.jsonl"
# data_indices_path = "benchmark/recceval/test_indices.json"

[query]
model_max_token_n = 2048
logits_token_n = 5
chunk_size = 3
# Используем модель с Hugging Face (будет скачана автоматически)
model_name_or_path = "Salesforce/codet5p-220m"
output_file = "cache/cceval/query/logits-m2048-n5-c3.json"
method = "logits"
lask_k = 10


[retrieve]
use_query_file = "cache/cceval/query/logits-m2048-n5-c3.json"
output_file = "cache/cceval/retrieve/sparse-dense-dataflow.json"

[retrieve.sparse]
method = "tfidf"
enable = true
var_topk = 5
func_topk = 5

[retrieve.dense]
# Используем модель с Hugging Face
emb_model = "Salesforce/codet5p-110m-embedding"
enable = true
var_topk = 5
func_topk = 5

[retrieve.dataflow]
enable = true
graph_use_cache = true
graph_cache_dir = "cache/cceval/dataflow/graphs"

[rerank]
use_retrieve_file = "cache/cceval/retrieve/sparse-dense-dataflow.json"
use_dense = true
use_sparse = true
use_dataflow = true
output_file = "cache/cceval/rerank/reranked-top10.json"
enable = true
think = false
method = "local"
sort_method = "heap"
bubble_window = 6
bubble_step = 3
heap_child_n = 5
# Настройте API URL для вашего LLM сервера (vLLM или OpenAI-compatible)
rerank_api_url = "http://localhost:8000/v1"
rerank_model = "Qwen/Qwen2.5-Coder-7B-Instruct"
rerank_api_key = "sk-xxx"
top_k = 10

[rerank.distill]
use_retrieve_file = "cache/cceval/retrieve/sparse-dense-dataflow.json"
retrieve_data_indices_path = "benchmark/cceval/test_indices.json"
train_data_output_file = "cache/cceval/rerank/distill/training_data.json"
choice_num_list = [2,3,4,5,6,7]
each_kind_num = 3
teacher_model_name = "Qwen2.5-Coder-7B-Instruct"
teacher_model_api_url = "http://localhost:8000/v1"
teacher_model_api_key = "sk-xxx"
training_base_model_path_or_name = "Qwen/Qwen2.5-0.5B"
use_training_data_path = "cache/cceval/rerank/distill/training_data.json"
checkpoint_save_path = "cache/checkpoints"

[rerank.human_eval]
use_rerank_file = "cache/cceval/rerank/reranked-top10.json"
rerank_data_indices_path = "benchmark/cceval/test_indices.json"
output_file = "cache/cceval/rerank/human_eval.json"

[build_prompt]
use_retrieval = true
use_retrieve_file = "cache/cceval/retrieve/sparse-dense-dataflow.json"
use_rerank_file = "cache/cceval/rerank/reranked-top10.json"
output_file = "cache/cceval/prompt/prompts.json"
max_token_n = 2000
use_rerank_k = 10
tokenizer_path_or_name = "Qwen/Qwen2.5-Coder-7B-Instruct"


[inference]
use_prompt_file = "cache/cceval/prompt/prompts.json"
output_file = "cache/cceval/inference/completions.json"
max_tokens = 48
# Настройте API URL для вашего LLM сервера
api_url = "http://localhost:8000/v1"
api_key = "sk-xxx"
model = "Qwen/Qwen2.5-Coder-7B-Instruct"

[evaluation]
use_inference_file = "cache/cceval/inference/completions.json"
output_file = "cache/cceval/evaluation/results.json"
# inference_data_indices_path = "benchmark/cceval/test_indices.json"
